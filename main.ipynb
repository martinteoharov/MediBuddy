{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD95pZ7NoGGz"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "# Put all 'pip install' commands here..\n",
        "\n",
        "!pip install torch transformers pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "device = \"cpu\"\n",
        "# Move the model to the GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"Paris is the capital of France.\"\n",
        "\n",
        "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors='pt')\n",
        "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "attention_mask = inputs[\"attention_mask\"].tolist()[0]\n",
        "\n",
        "input_ids = torch.tensor([input_ids]).to(device)\n",
        "attention_mask = torch.tensor([attention_mask]).to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "\n",
        "if answer_end >= answer_start:\n",
        "    answer_tokens = input_ids[0][answer_start:answer_end+1].tolist()\n",
        "    answer_tokens = tokenizer.convert_ids_to_tokens(answer_tokens)\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "\n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5ADGhD3rP9i",
        "outputId": "e78aa1a1-0c20-49bd-bb36-2b423ca08584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output paths\n",
        "import os\n",
        "os.makedirs('./results', exist_ok = True)\n",
        "OUTPUT_DIR: str = './results'"
      ],
      "metadata": {
        "id": "ijlyoUThUZ6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign values to few params \n",
        "MODEL_NAME = 'EleutherAI/gpt-neo-125M'\n",
        "BOS_TOKEN = '<|startoftext|>'\n",
        "EOS_TOKEN = '<|endoftext|>'\n",
        "PAD_TOKEN = '<|pad|>'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n",
        "\n",
        "model =  AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "DATA_PATH = '/kaggle/input/diagnoise-me/diagnose_en_dataset.feather'\n",
        "\n",
        "data = pd.read_feather(DATA_PATH)\n",
        "data = data['Patient'].values\n",
        "\n",
        "SEQ_LEN = 1024\n",
        "SAMPLE_SIZE =  int(data.shape[0] * 0.01)\n",
        "_data = [el[:SEQ_LEN]  for el in data[:SAMPLE_SIZE]]\n",
        "\n",
        "dataset = PatientDiagnozeDataset(txt_list = _data, tokenizer = tokenizer, max_length = 1024)\n",
        "\n",
        "TRAIN_SIZE = int(len(dataset) * 0.8)\n",
        "train_dataset, val_dataset = random_split(dataset, [TRAIN_SIZE, len(dataset) - TRAIN_SIZE])\n",
        "\n",
        "training_args = TrainingArguments(output_dir = OUTPUT_DIR, num_train_epochs = 2, logging_steps = 5000, \n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  per_device_train_batch_size=2, per_device_eval_batch_size=2, \n",
        "                                  warmup_steps=50, weight_decay=0.01, logging_dir='./logs', \n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                 load_best_model_at_end=True)\n",
        "\n",
        "_trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                                              'labels': torch.stack([f[0] for f in data])})\n",
        "_trainer.train()"
      ],
      "metadata": {
        "id": "QPem3mjtsgCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated = tokenizer(BOS_TOKEN, return_tensors=\"pt\").input_ids.cuda()"
      ],
      "metadata": {
        "id": "nOXRx802UhWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = model.generate(generated, do_sample=True, top_k=50,\n",
        "                                bos_token='<|startoftext|>',\n",
        "                                eos_token='<|endoftext|>', pad_token='<|pad|>',\n",
        "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "opO6sskGUjCt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}