{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Diagnose me "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Diagnose me is an LFQA dataset of dialogues between patients and doctors based on factual conversations from icliniq.com and healthcaremagic.com that aims to collect more than 257k of different questions and prescriptions for patients."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src ='https://plus.unsplash.com/premium_photo-1661281252401-f03c9bfb6925?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80'>"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-29T22:42:07.241248Z","iopub.status.busy":"2023-01-29T22:42:07.240515Z","iopub.status.idle":"2023-01-29T22:42:07.276651Z","shell.execute_reply":"2023-01-29T22:42:07.275668Z","shell.execute_reply.started":"2023-01-29T22:42:07.240835Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Description"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["https://huggingface.co/EleutherAI/gpt-neo-1.3B\n","\n","https://huggingface.co/EleutherAI/gpt-neo-125M\n","\n","Model Description\n","\n","GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.\n","Training data\n","\n","GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.\n","Training procedure\n","\n","This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.\n","Intended Use and Limitations\n","\n","This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Main task of this notebook\n","\n","In this notebook I wanted to show how to simply fine tune Any Gpt model and use it in own area-dependent use case. \n","Patient/doctor diagnosis seems to be very intriguing in case of generating phrases by Gpt Neo 1.3b model. Let's see how it can be done, and think of potential usage of that kind od model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Imports \n","Main libraries used in these project are as followe, \n","this notebook is mainly based on transformers library and torch. \n","In this notebook you'll know how to properly define dataset using Dataset torch module, and how to use few transformers functionalities to fit any model to your own usage. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:42:07.279222Z","iopub.status.busy":"2023-01-29T22:42:07.278588Z","iopub.status.idle":"2023-01-29T22:42:09.072505Z","shell.execute_reply":"2023-01-29T22:42:09.071517Z","shell.execute_reply.started":"2023-01-29T22:42:07.279186Z"},"trusted":true},"outputs":[],"source":["import pandas as pd \n","import torch\n","from torch.utils.data import Dataset, random_split\n","from typing import List, Dict, Union\n","from typing import Any, TypeVar"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:42:09.074673Z","iopub.status.busy":"2023-01-29T22:42:09.074107Z","iopub.status.idle":"2023-01-29T22:42:15.537717Z","shell.execute_reply":"2023-01-29T22:42:15.536612Z","shell.execute_reply.started":"2023-01-29T22:42:09.074634Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, TrainingArguments \n","from transformers import Trainer, AutoModelForCausalLM, IntervalStrategy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:42:15.541902Z","iopub.status.busy":"2023-01-29T22:42:15.540835Z","iopub.status.idle":"2023-01-29T22:42:15.55592Z","shell.execute_reply":"2023-01-29T22:42:15.554744Z","shell.execute_reply.started":"2023-01-29T22:42:15.541868Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(2137)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data processing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:42:15.557964Z","iopub.status.busy":"2023-01-29T22:42:15.557605Z","iopub.status.idle":"2023-01-29T22:42:15.571211Z","shell.execute_reply":"2023-01-29T22:42:15.570262Z","shell.execute_reply.started":"2023-01-29T22:42:15.557929Z"},"trusted":true},"outputs":[],"source":["# Assign values to few params \n","MODEL_NAME: str = 'EleutherAI/gpt-neo-125M'\n","BOS_TOKEN: str = '<|startoftext|>'\n","EOS_TOKEN: str = '<|endoftext|>'\n","PAD_TOKEN: str = '<|pad|>'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- BOS Token - beginning of sentence token \n","- EOS Token - end of sentence token \n","- PAD Token - adding adds a special padding token to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length ..."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Tokenizer read"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:42:15.573142Z","iopub.status.busy":"2023-01-29T22:42:15.572626Z","iopub.status.idle":"2023-01-29T22:42:34.089664Z","shell.execute_reply":"2023-01-29T22:42:34.088697Z","shell.execute_reply.started":"2023-01-29T22:42:15.573108Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, \n","                                          eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Read model and resize token embedding to length of tokenizer object"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:42:34.091683Z","iopub.status.busy":"2023-01-29T22:42:34.091209Z","iopub.status.idle":"2023-01-29T22:43:33.358249Z","shell.execute_reply":"2023-01-29T22:43:33.3573Z","shell.execute_reply.started":"2023-01-29T22:42:34.091642Z"},"trusted":true},"outputs":[],"source":["model =  AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n","model.resize_token_embeddings(len(tokenizer))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load dataset \n","\n","Our dataset is relatively simple to read, at the starter to focus on Patient questions, and read data which is saved in .feather format. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:33.360461Z","iopub.status.busy":"2023-01-29T22:43:33.35953Z","iopub.status.idle":"2023-01-29T22:43:33.365039Z","shell.execute_reply":"2023-01-29T22:43:33.36407Z","shell.execute_reply.started":"2023-01-29T22:43:33.360423Z"},"trusted":true},"outputs":[],"source":["DATA_PATH: str = '/kaggle/input/diagnoise-me/diagnose_en_dataset.feather'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Define maximum length and make input dataset little bit shorter to avoid computation complexity"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:33.367292Z","iopub.status.busy":"2023-01-29T22:43:33.366449Z","iopub.status.idle":"2023-01-29T22:43:35.391885Z","shell.execute_reply":"2023-01-29T22:43:35.390921Z","shell.execute_reply.started":"2023-01-29T22:43:33.367258Z"},"trusted":true},"outputs":[],"source":["data = pd.read_feather(DATA_PATH)\n","data = data['Patient'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:35.396532Z","iopub.status.busy":"2023-01-29T22:43:35.395586Z","iopub.status.idle":"2023-01-29T22:43:35.402732Z","shell.execute_reply":"2023-01-29T22:43:35.401765Z","shell.execute_reply.started":"2023-01-29T22:43:35.396493Z"},"trusted":true},"outputs":[],"source":["SEQ_LEN: int = 1024\n","SAMPLE_SIZE: int =  int(data.shape[0] * 0.01) #Just get only .01 fraction of els\n","_data = [el[:SEQ_LEN]  for el in data[:SAMPLE_SIZE]]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Torch Dataloader \n","- Simple Dataloader which allows us to modify input data, add element from tokenizer to proper object which are responsible for  input_ids, attention_masks and also labels. \n","- Also, a little processing which is based on adding BOS, EOS, passing token, defined above"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:35.406298Z","iopub.status.busy":"2023-01-29T22:43:35.40602Z","iopub.status.idle":"2023-01-29T22:43:35.41428Z","shell.execute_reply":"2023-01-29T22:43:35.413262Z","shell.execute_reply.started":"2023-01-29T22:43:35.406273Z"}},"source":["class PatientDiagnozeDataset(Dataset):\n","    \n","    def __init__(self, txt_list, tokenizer, max_length):\n","        \n","        self.input_ids: List = []\n","        self.attn_masks = []\n","        self.labels = []\n","        for txt in txt_list:\n","            encodings_dict = tokenizer(BOS_TOKEN + txt + EOS_TOKEN, truncation=True, \n","                                      max_length = max_length, padding = \"max_length\")\n","            self.input_ids.append(torch.tensor(encodings_dict[\"input_ids\"]))\n","            self.attn_masks.append(torch.tensor(encodings_dict[\"attention_mask\"]))\n","            \n","    def __len__(self):\n","        return len(self.input_ids)\n","    \n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:35.416858Z","iopub.status.busy":"2023-01-29T22:43:35.416016Z","iopub.status.idle":"2023-01-29T22:43:46.693638Z","shell.execute_reply":"2023-01-29T22:43:46.692697Z","shell.execute_reply.started":"2023-01-29T22:43:35.41682Z"},"trusted":true},"outputs":[],"source":["# Load tokenizer \n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, bos_token = BOS_TOKEN, \n","                                         eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load Dataset\n","\n","- define size of training dataset\n","- define size of validation dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:46.695633Z","iopub.status.busy":"2023-01-29T22:43:46.695068Z","iopub.status.idle":"2023-01-29T22:43:48.642357Z","shell.execute_reply":"2023-01-29T22:43:48.641343Z","shell.execute_reply.started":"2023-01-29T22:43:46.695594Z"},"trusted":true},"outputs":[],"source":["dataset = PatientDiagnozeDataset(txt_list = _data, tokenizer = tokenizer, max_length = 1024)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:48.649277Z","iopub.status.busy":"2023-01-29T22:43:48.64706Z","iopub.status.idle":"2023-01-29T22:43:48.657043Z","shell.execute_reply":"2023-01-29T22:43:48.656211Z","shell.execute_reply.started":"2023-01-29T22:43:48.649238Z"},"trusted":true},"outputs":[],"source":["TRAIN_SIZE: int = int(len(dataset) * 0.8)\n","train_dataset, val_dataset = random_split(dataset, [TRAIN_SIZE, len(dataset) - TRAIN_SIZE])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Create output paths\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:48.664416Z","iopub.status.busy":"2023-01-29T22:43:48.66164Z","iopub.status.idle":"2023-01-29T22:43:48.670778Z","shell.execute_reply":"2023-01-29T22:43:48.669827Z","shell.execute_reply.started":"2023-01-29T22:43:48.664355Z"},"trusted":true},"outputs":[],"source":["import os\n","os.makedirs('./results', exist_ok = True)\n","OUTPUT_DIR: str = './results'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Define training arguments \n","- num_train_epochs - number of epochs of training process,\n","- per_device_train_batch_size -  batch size in case of using 1 gpu\n","- warmup steps - Warm-up is a way to reduce the primacy effect of the early training examples. Without it, you may need to run a few extra epochs to get the convergence desired, as the model un-trains those early superstitions.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:48.677577Z","iopub.status.busy":"2023-01-29T22:43:48.675596Z","iopub.status.idle":"2023-01-29T22:43:48.686107Z","shell.execute_reply":"2023-01-29T22:43:48.685152Z","shell.execute_reply.started":"2023-01-29T22:43:48.677543Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(output_dir = OUTPUT_DIR, num_train_epochs = 2, logging_steps = 5000, \n","                                  save_strategy=\"epoch\",\n","                                  per_device_train_batch_size=2, per_device_eval_batch_size=2, \n","                                  warmup_steps=50, weight_decay=0.01, logging_dir='./logs', \n","                                  evaluation_strategy=\"epoch\",\n","                                 load_best_model_at_end=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Define trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T22:43:48.693008Z","iopub.status.busy":"2023-01-29T22:43:48.690781Z","iopub.status.idle":"2023-01-29T23:02:23.36688Z","shell.execute_reply":"2023-01-29T23:02:23.365343Z","shell.execute_reply.started":"2023-01-29T22:43:48.692971Z"},"trusted":true},"outputs":[],"source":["_trainer =Trainer(model=model, args=training_args, train_dataset=train_dataset,\n","        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n","                                                              'attention_mask': torch.stack([f[1] for f in data]),\n","                                                              'labels': torch.stack([f[0] for f in data])})\n","_trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T23:02:34.176688Z","iopub.status.busy":"2023-01-29T23:02:34.176288Z","iopub.status.idle":"2023-01-29T23:02:34.184507Z","shell.execute_reply":"2023-01-29T23:02:34.183488Z","shell.execute_reply.started":"2023-01-29T23:02:34.176657Z"},"trusted":true},"outputs":[],"source":["generated = tokenizer(BOS_TOKEN, return_tensors=\"pt\").input_ids.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T23:02:36.617145Z","iopub.status.busy":"2023-01-29T23:02:36.616786Z","iopub.status.idle":"2023-01-29T23:02:42.015364Z","shell.execute_reply":"2023-01-29T23:02:42.014275Z","shell.execute_reply.started":"2023-01-29T23:02:36.617114Z"},"trusted":true},"outputs":[],"source":["sample_outputs = model.generate(generated, do_sample=True, top_k=50,\n","                                bos_token='<|startoftext|>',\n","                                eos_token='<|endoftext|>', pad_token='<|pad|>',\n","                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
